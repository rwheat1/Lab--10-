---
title: "Lab 10 - Grading the professor, Pt. 2"
author: "Insert your name here"
date: "Insert date here"
output: github_document
---

### Load packages and data

```{r load-packages, message=FALSE}
library(tidyverse) 
library(tidymodels)
library(openintro)
```

### Exercise 1

A one unit increase in a professor's beauty rating leads to a .06 unit increase in their course evaluations. R2 = 3.5%; Adjusted R2 = 3.3%.

```{r model-one}

m_bty <- linear_reg() %>%
  set_engine ("lm") %>%
  fit(score ~ bty_avg, data = evals)

tidy(m_bty)

glance(m_bty)$r.squared
glance(m_bty)$adj.r.squared

```


### Exercise 2

score = bty_avg(.074) + gender(.172)

R2 = 5.9%
Adjusted R2 = 5.5%

```{r model-two}

m_bty_gen <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + gender, data = evals)

tidy(m_bty_gen)

glance(m_bty_gen)$r.squared
glance(m_bty_gen)$adj.r.squared

```


### Exercise 3

b1: Holding gender constant, a one unit increase in a professor's beauty rating predicts a .07 unit increase in their course evaluations. 

b2: Holding beauty constant, male professors tend to receive course evaluations that are .17 points higher than female professors.

Intercept: A female professor with a beauty rating of 0 will receive a 3.74 rating in their course evals.


### Exercise 4

5.5% of the total variance in course evaluation scores is explained by the professor's beauty and gender.

### Exercise 5

Equation for the Line for Male Professors:
y = 3.91 + .07(bty_avg)

### Exercise 6

Holding beauty constant, male professors tend to have higher course evaluations than female professors.

### Exercise 7

The relationship between beauty and course evaluations is stronger for male professors than female professors, over and above beauty and gender themselves as predictors. In other words, when a professor is male, beauty matters more for determining their course evaluations.

```{r interaction}

#create an interaction term

evals <- evals %>%
  mutate(bty_mcentered = bty_avg - mean(bty_avg))

m_bty_gen_int <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + gender + bty_mcentered*gender, data = evals)

tidy(m_bty_gen_int)
glance(m_bty_gen_int)$adj.r.squared
```


### Exercise 8

The r-squared values are larger when considering gender (R2 = .055) in addition to beauty, rather than beauty as the sole predictor of course evaluations (R2 = .032). This suggests that there is unique variance explained by gender above and beyond what is explained by beauty.

### Exercise 9

m_bty: b = .066
m_bty_gen: b = .074

Adding gender as a predictor has increased the slope of beauty as a predictor of course evaluations.

### Exercise 10

y = bty_avg(.064) + rank_ten(-.04)

In order to interpret rank as a predictor, I turned that variable into a from three categorical options (tenured, tenure track, and teaching) to a dichotomous categorical variable: tenured professors, vs. non-tenured professors.

Holding rank constant, a one point increase in a professor's beauty score leads to a .064 increase in their course evaluations.

Holding beauty constant, there is no significant effect of tenure on course evaluations.

```{r rank}

evals <- evals %>%
  mutate(rank_ten = case_when(rank == "tenure track" ~ "non-tenured", rank == "teaching" ~ "non-tenured", rank == "tenured" ~ "tenured"))

m_bty_rank <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + rank_ten, data = evals)

tidy(m_bty_rank)
glance(m_bty_rank)$adj.r.squared
```

## Part 3

### Exercise 11

Of the variables in this dataframe, I would expect cls_credits to not be associated with professor evaluations at all. I just don't see any way how the number of credits a class is worth would impact how people viewed the professor in the course. Furthermore, most classes are multicredit, so I'm not sure much variance will be explained by variance in cls_credits.

### Exercise 12

I seem to be wrong here. Classes with one credit received higher course evaluations (b = .047) than classes that count for multiple credits. This variable alone explains 4% of the variance in course evalutions.

```{r cls-credit}

m_bty_cred <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ cls_credits, data = evals)

tidy(m_bty_cred)
glance(m_bty_cred)$adj.r.squared
```

### Exercise 13

If you are going to include cls_perc_eval and cls_students, there is no need to include cls_did_eval because the information you'd receive from that variable is redundant with the information provided by those other two variables. This may lead to quite a bit of collinearity, which can make our results a bit weird.

### Exercise 14

```{r full-model}

m_bty_full <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ rank + ethnicity + gender + language + age + cls_perc_eval + cls_students + cls_level + cls_profs + cls_credits + bty_avg, data = evals)

tidy(m_bty_full)
glance(m_bty_full)$adj.r.squared
```

### Exercise 15

After playing around with different sets of predictors, I've settled on the following model that is much simpler than the full model, but does not lose predictive power (and in fact, the adjusted R2 is larger).

y = ethnicity(.20) + gender(.18) + language(-.16) + age(-.005) + cls_percent_eval(.005) + cls_credits(.51) + bty_avg(.06)

```{r backwards-select}

m_bty_full <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ ethnicity + gender + language + age + cls_perc_eval + cls_credits + bty_avg, data = evals)

tidy(m_bty_full)
glance(m_bty_full)$adj.r.squared

```

### Exercise 16

Categorical variable interpretation: Holding all other predictors constant, professors for which English is not their native language tend to receive .16 worse course eval scores than those who are native English speakers.

Continuous variable interpretation: Holding all other predictors constant, a one unit increase in a professor's average beauty rating is associated with a .06 increase in their corresponding course evaluation scores.

### Exercise 17

According to our models, the professor most likely to receive positive course evaluations would be (looking at the betas for the significant predictors):
1. Not a minority
2. Male
3. Young
4. Attractive

The class would be:
1. One credit

### Exercise 18

I would be tentatively willing to generalize most of these results to other universities. Certainly, the biases that lead the characteristics of these professors to be valued are not specific to the University of Texas. However, I would be willing to bet that there may be additional idiosyncratic factors that may matter for determining course evaluations depending on the institution. For instance, courses are structured differently at different institutions: some places do not go by the traditional semester-based academic calendar, others do. How might this impact evaluations? It's unclear. Thus, I think the current significant predictors matter for course evaluations at other universities, but there may be additional things that matter at those other schools as well.
